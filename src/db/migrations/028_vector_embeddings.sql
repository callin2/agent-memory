-- ============================================================================
-- Migration 028: Vector Embeddings & Semantic Search
-- ============================================================================
-- Task: https://github.com/callin/agent_memory_v2/issues/16
-- Expert: Database + ML
-- Priority: P2/MEDIUM (search quality)
-- Effort: 16-20 hours (2-2.5 days)
-- Impact: 2-3× better retrieval relevance
--
-- Problem:
-- - Full-text search (FTS) matches keywords, not semantic meaning
-- - "dog" doesn't match "puppy" (different words, same concept)
-- - Can't find similar concepts expressed differently
--
-- Solution:
-- 1. Add pgvector extension for vector similarity
-- 2. Generate embeddings using OpenAI text-embedding-3-small
-- 3. Store embeddings in session_handoffs and semantic_memory
-- 4. Replace FTS with vector similarity search (cosine similarity)
--
-- Research Basis:
-- - Word2Vec (Mikolov, 2013): Words as vectors
-- - BERT (Devlin, 2018): Contextual embeddings
-- - Sentence Transformers (Reimers, 2019): Semantic similarity
--
-- ============================================================================

-- Enable pgvector extension
CREATE EXTENSION IF NOT EXISTS vector;

-- ============================================================================
-- Add embedding columns to session_handoffs
-- ============================================================================

ALTER TABLE session_handoffs
  ADD COLUMN IF NOT EXISTS embedding vector(1536); -- text-embedding-3-small = 1536 dimensions

COMMENT ON COLUMN session_handoffs.embedding IS
  'Vector embedding for semantic similarity search. Generated by OpenAI text-embedding-3-small.';

-- ============================================================================
-- Add embedding columns to semantic_memory
-- ============================================================================

ALTER TABLE semantic_memory
  ADD COLUMN IF NOT EXISTS embedding vector(1536);

COMMENT ON COLUMN semantic_memory.embedding IS
  'Vector embedding for semantic principle similarity search.';

-- ============================================================================
-- Create vector similarity index (HNSW - Hierarchical Navigable Small World)
-- ============================================================================

-- Index for session_handoffs (fast approximate nearest neighbor search)
CREATE INDEX IF NOT EXISTS idx_session_handoffs_embedding_hnsw
  ON session_handoffs USING hnsw (embedding vector_cosine_ops)
  WITH (m = 16, ef_construction = 64);

COMMENT ON INDEX idx_session_handoffs_embedding_hnsw IS
  'HNSW index for fast approximate vector similarity search. m=16, ef_construction=64.';

-- Index for semantic_memory
CREATE INDEX IF NOT EXISTS idx_semantic_memory_embedding_hnsw
  ON semantic_memory USING hnsw (embedding vector_cosine_ops)
  WITH (m = 16, ef_construction = 64);

COMMENT ON INDEX idx_semantic_memory_embedding_hnsw IS
  'HNSW index for semantic memory vector similarity search.';

-- ============================================================================
-- Vector similarity search functions
-- ============================================================================

-- Function to find semantically similar handoffs
-- Returns top K most similar sessions by cosine similarity
CREATE OR REPLACE FUNCTION find_semantically_similar_handoffs(
  p_tenant_id TEXT,
  p_embedding vector(1536),
  p_limit INT DEFAULT 5,
  p_min_similarity FLOAT DEFAULT 0.5
)
RETURNS TABLE (
  handoff_id TEXT,
  tenant_id TEXT,
  experienced TEXT,
  learned TEXT,
  becoming TEXT,
  similarity FLOAT,
  created_at TIMESTAMPTZ
) AS $$
BEGIN
  RETURN QUERY
  SELECT
    sh.handoff_id,
    sh.tenant_id,
    sh.experienced,
    sh.learned,
    sh.becoming,
    1 - (sh.embedding <=> p_embedding) AS similarity, -- Cosine distance → similarity
    sh.created_at
  FROM session_handoffs sh
  WHERE sh.tenant_id = p_tenant_id
    AND sh.embedding IS NOT NULL
    AND (sh.embedding <=> p_embedding) < (1 - p_min_similarity) -- Cosine distance < threshold
  ORDER BY sh.embedding <=> p_embedding  -- Cosine distance (smaller = more similar)
  LIMIT p_limit;
END;
$$ LANGUAGE plpgsql;

COMMENT ON FUNCTION find_semantically_similar_handoffs IS
  'Find semantically similar sessions using vector cosine similarity. Returns top K results.';

-- ============================================================================
-- Migration function to generate embeddings for existing data
-- ============================================================================
--
-- This function is called by the embedding generation service
-- NOT run automatically in migration (would require OpenAI API key)
--
-- Usage:
--   SELECT generate_embeddings_for_tenant('default');
--
-- ============================================================================

-- Function to mark handoffs for embedding generation
CREATE OR REPLACE FUNCTION mark_handoffs_for_embedding(
  p_tenant_id TEXT DEFAULT 'default',
  p_batch_size INT DEFAULT 100
)
RETURNS INT AS $$
DECLARE
  v_count INT;
BEGIN
  -- Update handoffs that don't have embeddings yet
  -- Mark them with a flag so the batch processor can find them
  UPDATE session_handoffs
  SET tags = array_distinct(tags || ARRAY['_needs_embedding'])
  WHERE tenant_id = p_tenant_id
    AND embedding IS NULL
    AND compression_level = 'full'
  LIMIT p_batch_size;

  GET DIAGNOSTICS v_count = ROW_COUNT;

  RETURN v_count;
END;
$$ LANGUAGE plpgsql;

COMMENT ON FUNCTION mark_handoffs_for_embedding IS
  'Mark handoffs that need embeddings. Used by batch embedding processor.';

-- Function to check embedding progress
CREATE OR REPLACE FUNCTION get_embedding_progress(p_tenant_id TEXT DEFAULT 'default')
RETURNS TABLE (
  total_handoffs INT,
  with_embeddings INT,
  without_embeddings INT,
  progress_percent FLOAT
) AS $$
BEGIN
  RETURN QUERY
  SELECT
    COUNT(*) AS total_handoffs,
    COUNT(*) FILTER (WHERE embedding IS NOT NULL) AS with_embeddings,
    COUNT(*) FILTER (WHERE embedding IS NULL) AS without_embeddings,
    CASE
      WHEN COUNT(*) > 0 THEN
        (COUNT(*) FILTER (WHERE embedding IS NOT NULL)::FLOAT / COUNT(*) * 100)
      ELSE 0
    END AS progress_percent
  FROM session_handoffs
  WHERE tenant_id = p_tenant_id;
END;
$$ LANGUAGE plpgsql;

COMMENT ON FUNCTION get_embedding_progress IS
  'Check embedding generation progress for a tenant.';

-- ============================================================================
-- Performance Comparison: FTS vs Vector Search
-- ============================================================================
--
-- Full-Text Search (Current):
-- - Matches exact keywords
-- - "dog" doesn't match "puppy"
-- - Fast but semantically limited
-- - Index: GIN on tsvector
--
-- Vector Search (New):
-- - Matches semantic meaning
-- - "dog" matches "puppy", "canine", "pet"
-- - Slower but semantically rich
-- - Index: HNSW (approximate nearest neighbor)
--
-- Hybrid Approach (Best of Both):
-- - Use FTS for keyword-only queries
-- - Use Vector for semantic similarity queries
-- - Combine results with Reciprocal Rank Fusion (RRF)
--
-- ============================================================================

-- ============================================================================
-- Usage Examples
-- ============================================================================
--
-- 1. Generate embeddings (via service, not migration):
--    const embedding = await openai.embeddings.create({
--      model: "text-embedding-3-small",
--      input: experienced + " " + learned + " " + becoming
--    });
--
-- 2. Store embedding:
--    UPDATE session_handoffs
--    SET embedding = '[...1536 dimensions...]'
--    WHERE handoff_id = '...';
--
-- 3. Semantic search:
--    SELECT * FROM find_semantically_similar_handoffs(
--      'default',
--      '[...query embedding...]',
--      5,
--      0.7
--    );
--
-- 4. Cosine similarity formula:
--    similarity = 1 - cosine_distance
--    cosine_distance = embedding1 <=> embedding2
--    Range: 0 (opposite) to 1 (identical)
--
-- ============================================================================
